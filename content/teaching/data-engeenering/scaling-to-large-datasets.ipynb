{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9987c6fb",
   "metadata": {},
   "source": [
    "# Scaling to large datasets\n",
    "\n",
    "Pandas provides data structures for **in-memory analytics**, which makes using pandas to analyze datasets that are larger than memory datasets somewhat tricky.\n",
    "Even datasets that are a sizable fraction of memory become unwieldy, as some pandas operations need to make intermediate copies.\n",
    "\n",
    "The document [Scaling to large datasets – Pandas User Guide](https://pandas.pydata.org/docs/user_guide/scale.html) provides a few recommendations for scaling your analysis to larger datasets. It’s a complement to Enhancing performance, which focuses on speeding up analysis for datasets that fit in memory.\n",
    "\n",
    "The documentation shows a few strategies for scaling to larger datasets:\n",
    "\n",
    "- Load less data\n",
    "- Use efficient datatypes\n",
    "- **Use chunking**\n",
    "- Use Other Libraries\n",
    "\n",
    "In this notebook, we focus on **chunking** and present a more advanced example than the one shown in the official documentation. Here, we demonstrate how to process data in smaller pieces that fit into memory and perform chunk aggregation for a slightly more complex data processing task then the one shown in the documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc042556",
   "metadata": {},
   "source": [
    "## File chunking with TextFileReader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75815d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "ds_100k_path = Path('.data') / '100k.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "990cd98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.io.parsers.readers.TextFileReader"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfr  = pd.read_csv(ds_100k_path, chunksize=100) \n",
    "\n",
    "type(tfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "666c4497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 200 to 299\n",
      "Data columns (total 14 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Region          100 non-null    object \n",
      " 1   Country         100 non-null    object \n",
      " 2   Item Type       100 non-null    object \n",
      " 3   Sales Channel   100 non-null    object \n",
      " 4   Order Priority  100 non-null    object \n",
      " 5   Order Date      100 non-null    object \n",
      " 6   Order ID        100 non-null    int64  \n",
      " 7   Ship Date       100 non-null    object \n",
      " 8   Units Sold      100 non-null    int64  \n",
      " 9   Unit Price      100 non-null    float64\n",
      " 10  Unit Cost       100 non-null    float64\n",
      " 11  Total Revenue   100 non-null    float64\n",
      " 12  Total Cost      100 non-null    float64\n",
      " 13  Total Profit    100 non-null    float64\n",
      "dtypes: float64(5), int64(2), object(7)\n",
      "memory usage: 11.1+ KB\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(tfr):\n",
    "    chunk.info()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c5c04e",
   "metadata": {},
   "source": [
    "## Usage example: calculating the mean value of a column\n",
    "\n",
    "Let us assume we would like to calculate the mean value of some column.\n",
    "\n",
    "In the classic approach, one would use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bb6c98d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 14 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   Region          100000 non-null  object \n",
      " 1   Country         100000 non-null  object \n",
      " 2   Item Type       100000 non-null  object \n",
      " 3   Sales Channel   100000 non-null  object \n",
      " 4   Order Priority  100000 non-null  object \n",
      " 5   Order Date      100000 non-null  object \n",
      " 6   Order ID        100000 non-null  int64  \n",
      " 7   Ship Date       100000 non-null  object \n",
      " 8   Units Sold      100000 non-null  int64  \n",
      " 9   Unit Price      100000 non-null  float64\n",
      " 10  Unit Cost       100000 non-null  float64\n",
      " 11  Total Revenue   100000 non-null  float64\n",
      " 12  Total Cost      100000 non-null  float64\n",
      " 13  Total Profit    100000 non-null  float64\n",
      "dtypes: float64(5), int64(2), object(7)\n",
      "memory usage: 10.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(ds_100k_path)\n",
    "\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eeedfe",
   "metadata": {},
   "source": [
    "Please note the memory usage of both DataFrames (last line in the load output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac2ab514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Region\n",
       "Asia                                 265.792398\n",
       "Australia and Oceania                262.774268\n",
       "Central America and the Caribbean    268.143213\n",
       "Europe                               266.845405\n",
       "Middle East and North Africa         266.141459\n",
       "North America                        270.677000\n",
       "Sub-Saharan Africa                   267.651037\n",
       "Name: Unit Price, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_reference = df.groupby('Region')['Unit Price'].mean()\n",
    "\n",
    "s_reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebe8df7",
   "metadata": {},
   "source": [
    "However, if the dataset is too big and we cannot load it to the memory, \n",
    "we may use the cunks.\n",
    "\n",
    "In the example provided in the documentation, the goal is to calculate the value counts of a column.\n",
    "In such case, the final value counts can be obtained by summing up the value counts of each chunk.\n",
    "This procedure is called **chunk aggregation**.\n",
    "\n",
    "Coming back to out example of calculating the mean value:\n",
    "\n",
    "### **Question**: how to calculate the mean of all values while only having parts of the data.\n",
    "\n",
    "Answer: Mathematics B)\n",
    "\n",
    "We need to find (Google, GPT, Gemini, etc.) how to get the average of the whole group while having the averages of the subgroups:\n",
    "\n",
    "> The weighted average of subgroup averages is mathematically identical to the average of the whole group.\n",
    "\n",
    "So, we take the formula for the weighted average:\n",
    "\n",
    "$$\\text{Weighted Average} = \\frac{\\sum (\\text{frag\\_avg\\_grade} \\times \\text{frag\\_count})}{\\sum \\text{frag\\_count}}$$\n",
    "\n",
    "and implement the whole idea.\n",
    "\n",
    "**Note**: chunk aggregation is usually the hard part of the whole chunk computation – you need to figure out how to combine the results from each chunk to get the final result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9a24c9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 1000/1000.0 [00:02<00:00, 333.97chunk/s]\n"
     ]
    }
   ],
   "source": [
    "# note: re-create the TextFileReader it was used up above\n",
    "tfr  = pd.read_csv(\n",
    "    ds_100k_path,\n",
    "    chunksize=100\n",
    ")\n",
    "\n",
    "frag_stats = []\n",
    "for chunk in tqdm(tfr, total=100e3/100, desc='Processing chunks', unit='chunk'):\n",
    "    \n",
    "    # for the fragment (subgroup/chunk/etc.)...\n",
    "    df = chunk if type(chunk) == pd.DataFrame else chunk.to_frame() # series handling \n",
    "    df_frag_stats = df.groupby('Region').agg(\n",
    "        frag_avg_grade=('Unit Price', 'mean'),   # ... calculate the mean ...\n",
    "        frag_count=('Unit Price', 'count') \t# ... and the count (will be used as the weight).\n",
    "    )\n",
    "    frag_stats.append(df_frag_stats)  # Store only this aggregated data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7b1cb2",
   "metadata": {},
   "source": [
    "Concat it to one df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1f21132",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = pd.concat(frag_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21f2c73",
   "metadata": {},
   "source": [
    "And then apply the mathematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be0c24e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prod</th>\n",
       "      <th>frag_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Region</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Asia</th>\n",
       "      <td>3866482.02</td>\n",
       "      <td>14547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Australia and Oceania</th>\n",
       "      <td>2131887.64</td>\n",
       "      <td>8113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Central America and the Caribbean</th>\n",
       "      <td>2877444.82</td>\n",
       "      <td>10731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Europe</th>\n",
       "      <td>6905158.55</td>\n",
       "      <td>25877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Middle East and North Africa</th>\n",
       "      <td>3348059.55</td>\n",
       "      <td>12580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>North America</th>\n",
       "      <td>577354.04</td>\n",
       "      <td>2133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sub-Saharan Africa</th>\n",
       "      <td>6964012.32</td>\n",
       "      <td>26019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         prod  frag_count\n",
       "Region                                                   \n",
       "Asia                               3866482.02       14547\n",
       "Australia and Oceania              2131887.64        8113\n",
       "Central America and the Caribbean  2877444.82       10731\n",
       "Europe                             6905158.55       25877\n",
       "Middle East and North Africa       3348059.55       12580\n",
       "North America                       577354.04        2133\n",
       "Sub-Saharan Africa                 6964012.32       26019"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stats['prod'] = df_stats['frag_avg_grade'] * df_stats['frag_count']\n",
    "\n",
    "result = df_stats.groupby(level=0)[['prod', 'frag_count']].sum()\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2898683b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   weighted_avg\n",
      "Region                                         \n",
      "Asia                                 265.792398\n",
      "Australia and Oceania                262.774268\n",
      "Central America and the Caribbean    268.143213\n",
      "Europe                               266.845405\n",
      "Middle East and North Africa         266.141459\n",
      "North America                        270.677000\n",
      "Sub-Saharan Africa                   267.651037\n"
     ]
    }
   ],
   "source": [
    "result['weighted_avg'] = result['prod'] / result['frag_count']\n",
    "print(result[['weighted_avg']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e99f727",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(result['weighted_avg'].compare(s_reference).shape[0] == 0), \"Results differ!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd569c0",
   "metadata": {},
   "source": [
    "As we can see, the results are identical to the non-chunked approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840b2e49",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Chunking is a technique for processing large datasets by splitting them into smaller, manageable pieces (\"chunks\") that fit into memory. Instead of loading the entire dataset at once, you read and process each chunk sequentially.\n",
    "\n",
    "**Why use chunking?**\n",
    "- Enables analysis of datasets larger than available memory.\n",
    "- Reduces memory usage and risk of crashes.\n",
    "- Allows streaming and incremental processing.\n",
    "\n",
    "**Chunk aggregation** refers to combining results from each chunk to produce a final result (e.g., summing value counts, calculating weighted averages).\n",
    "\n",
    "**Advantages:**\n",
    "- Scales to very large datasets.\n",
    "- Efficient memory usage.\n",
    "- Can parallelize or distribute processing.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Requires careful aggregation logic (e.g., weighted averages, merging counts).\n",
    "- Some operations (sorting, global ranking) are harder or less efficient.\n",
    "- More complex code compared to simple in-memory operations.\n",
    "\n",
    "Chunking is essential for big data workflows, but chunk aggregation is often the challenging part, as you must design how to combine partial results correctly.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
